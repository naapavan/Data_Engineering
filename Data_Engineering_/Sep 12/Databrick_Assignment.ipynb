{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ebfd5ac-abcc-4dcf-9832-f161062dd166",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----------+-----------+------+\n|EmployeeID|         Name|Department|JoiningDate|Salary|\n+----------+-------------+----------+-----------+------+\n|      1001|     John Doe|        HR| 15-01-2021| 55000|\n|      1002|   Jane Smith|        IT| 10-03-2020| 62000|\n|      1003|Emily Johnson|   Finance| 01-07-2019| 70000|\n|      1004|Michael Brown|        HR| 22-12-2018| 54000|\n|      1005| David Wilson|        IT| 25-06-2021| 58000|\n|      1006|  Linda Davis|   Finance| 15-11-2020| 67000|\n|      1007| James Miller|        IT| 14-08-2019| 65000|\n|      1008|Barbara Moore|        HR| 29-03-2021| 53000|\n+----------+-------------+----------+-----------+------+\n\nroot\n |-- EmployeeID: string (nullable = true)\n |-- Name: string (nullable = true)\n |-- Department: string (nullable = true)\n |-- JoiningDate: string (nullable = true)\n |-- Salary: string (nullable = true)\n\n+----------+-------------+----------+-----------+------+\n|EmployeeID|         Name|Department|JoiningDate|Salary|\n+----------+-------------+----------+-----------+------+\n|      1001|     John Doe|        HR| 15-01-2021| 55000|\n|      1002|   Jane Smith|        IT| 10-03-2020| 62000|\n|      1004|Michael Brown|        HR| 22-12-2018| 54000|\n|      1005| David Wilson|        IT| 25-06-2021| 58000|\n|      1006|  Linda Davis|   Finance| 15-11-2020| 67000|\n|      1007| James Miller|        IT| 14-08-2019| 65000|\n|      1008|Barbara Moore|        HR| 29-03-2021| 53000|\n+----------+-------------+----------+-----------+------+\n\n+----------+-------------+----------+-----------+------+\n|EmployeeID|         Name|Department|JoiningDate|Salary|\n+----------+-------------+----------+-----------+------+\n|      1002|   Jane Smith|        IT| 10-03-2020| 62000|\n|      1003|Emily Johnson|   Finance| 01-07-2019| 70000|\n|      1005| David Wilson|        IT| 25-06-2021| 58000|\n|      1006|  Linda Davis|   Finance| 15-11-2020| 67000|\n|      1007| James Miller|        IT| 14-08-2019| 65000|\n+----------+-------------+----------+-----------+------+\n\n+----------+------------------+\n|Department|       avg(Salary)|\n+----------+------------------+\n|        HR|           54000.0|\n|   Finance|           68500.0|\n|        IT|61666.666666666664|\n+----------+------------------+\n\n+----------+-----+\n|Department|count|\n+----------+-----+\n|        HR|    3|\n|   Finance|    2|\n|        IT|    3|\n+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"Assignment\").getOrCreate()\n",
    "dbutils.fs.cp(\"File:/Workspace/Shared/assignment.csv\",\"dbfs:/FileStore/assignment.csv\")\n",
    "df=spark.read.format(\"csv\").option(\"header\",\"True\").load(\"dbfs:/FileStore/assignment.csv\")\n",
    "df.show()\n",
    "df.head(10)\n",
    "df.printSchema()\n",
    "df = df.withColumn(\"Salary\", df[\"Salary\"].cast(\"int\"))\n",
    "df_cleaned=df.filter(df.JoiningDate>\"1-01-2020\")\n",
    "df_cleaned.show()\n",
    "df_cleaned=df.filter(df.Salary>55000)\n",
    "df_cleaned.show()\n",
    "finding_avg=df.groupBy(\"Department\").avg(\"Salary\")\n",
    "finding_avg.show()\n",
    "find_count=df.groupBy(\"Department\").count()\n",
    "find_count.show()\n",
    "df_cleaned.write.format(\"csv\").mode(\"overwrite\").save(\"dbfs:/FileStore/cleaned_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa3d6e79-7e8b-4353-9e70-24d4303798d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Category: string (nullable = true)\n |-- Price: long (nullable = true)\n |-- ProductID: long (nullable = true)\n |-- ProductName: string (nullable = true)\n |-- Stock: long (nullable = true)\n\n+-----------+-----+---------+-----------+-----+\n|   Category|Price|ProductID|ProductName|Stock|\n+-----------+-----+---------+-----------+-----+\n|Electronics| 1200|      101|     Laptop|   35|\n|Electronics|  800|      102| Smartphone|   80|\n|  Furniture|  150|      103| Desk Chair|   60|\n|Electronics|  300|      104|    Monitor|   45|\n|  Furniture|  350|      105|       Desk|   25|\n+-----------+-----+---------+-----------+-----+\n\n+-----------+-----+---------+-----------+-----+\n|   Category|Price|ProductID|ProductName|Stock|\n+-----------+-----+---------+-----------+-----+\n|Electronics| 1200|      101|     Laptop|   35|\n|Electronics|  800|      102| Smartphone|   80|\n|  Furniture|  150|      103| Desk Chair|   60|\n|Electronics|  300|      104|    Monitor|   45|\n+-----------+-----+---------+-----------+-----+\n\n+-----------+-----+---------+-----------+-----+\n|   Category|Price|ProductID|ProductName|Stock|\n+-----------+-----+---------+-----------+-----+\n|Electronics| 1200|      101|     Laptop|   35|\n|Electronics|  800|      102| Smartphone|   80|\n|Electronics|  300|      104|    Monitor|   45|\n+-----------+-----+---------+-----------+-----+\n\n+-----------+----------+\n|   Category|sum(Stock)|\n+-----------+----------+\n|Electronics|       160|\n+-----------+----------+\n\n+-------------+\n|average Price|\n+-------------+\n|        560.0|\n+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "spark=SparkSession.builder.appName(\"jsonAssignment\").getOrCreate()\n",
    "dbutils.fs.cp(\"File:/Workspace/Shared/products.json\",\"dbfs:/FileStore/products.json\")\n",
    "df1=spark.read.format(\"json\").option(\"multiline\",\"True\").load(\"dbfs:/FileStore/products.json\")\n",
    "df1.printSchema()\n",
    "df1.head(10)\n",
    "df1.show()\n",
    "df_cleaned=df1.filter(df1.Stock>=30)\n",
    "df_cleaned.show()\n",
    "filet2=df1.filter(df1.Category==\"Electronics\")\n",
    "filet2.show()\n",
    "total_stocked=df1.groupBy(\"Category\").sum(\"Stock\")\n",
    "total_elec_stock=total_stocked.filter(total_stocked.Category==\"Electronics\")\n",
    "total_elec_stock.show()\n",
    "average_product_price=df1.agg(F.avg(\"Price\").alias(\"average Price\"))\n",
    "average_product_price.show()\n",
    "df_cleaned.write.format(\"json\").mode(\"overwrite\").save(\"dbfs:/FileStore/cleaned_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "788697fa-6541-4b94-aa14-4a83beb3e7ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n|col_name|data_type|comment|\n+--------+---------+-------+\n+--------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df_delta=df.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/FileStore/delta_emp_data\")\n",
    "df1.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/FileStore/delta_products_data\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS delta_emp_data USING DELTA LOCATION '/dbfs/FileStore/delta/delta_emp_data'\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS delta_products_data USING DELTA LOCATION '/dbfs/FileStore/delta/delta_products_data'\")\n",
    "spark.sql(\"DESCRIBE delta_emp_data\").show()\n",
    "df_finance_employees = spark.sql(\"SELECT * FROM employee_delta WHERE Department = 'Finance'\")\n",
    "df_finance_employees.show()\n",
    "df_expensive_electronics = spark.sql(\"SELECT * FROM product_delta WHERE Category = 'Electronics' AND Price > 500\")\n",
    "df_expensive_electronics.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "databricks assignment 2024-09-12 13:15:14",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
