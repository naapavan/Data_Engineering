{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFUzOv7ITAVe",
        "outputId": "a61adc3c-750e-4454-ba70-bc7cfa1ce98a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=f4d6adfba2818bb13a925eab364db0ea8300a60d4f368fc553edf64a95c31109\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import pandas as pd\n",
        "\n",
        "spark=SparkSession.builder.appName(\"coding_assesment\").getOrCreate()"
      ],
      "metadata": {
        "id": "LwiZt1PlTbwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"vehicle.csv\")\n",
        "dbutils.fs.cp(\"file:/content/vehicle.csv\",\"dbfs:/Filestore/vehicle.csv\")\n",
        "vehicle_df=spark.read.format(\"csv\").option(\"header\",\"true\").load(\"dbfs:/Filestore/vehicle.csv\")\n",
        "vehicle_df.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/Filestore/delta_vehicle\")\n",
        "\n",
        "try:\n",
        "                                                        vehcile_df=spark.read.format(\"delta\").load(\"dbfs:/Filestore/delta_vehicle\")\n",
        "except:\n",
        "      error as e:\n",
        "        print(e)\n",
        "\n"
      ],
      "metadata": {
        "id": "I9IB0ae_Tj7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#regstering a table\n",
        "%sql\n",
        "create table delta_vehicle using delta location \"dbfs:/Filestore/delta_vehicle\"\n",
        "\n",
        "# Read the registered Delta table\n",
        "df = spark.read.table(\"delta_vehicle\")\n",
        "\n",
        "# Perform cleaning operations (as shown earlier)\n",
        "df_cleaned = df.filter((df[\"ServiceCost\"] > 0) & (df[\"Mileage\"] > 0))\n",
        "df_cleaned = df_cleaned.dropDuplicates([\"VehicleID\", \"Date\"])\n",
        "\n",
        "# Save the cleaned data to a new Delta table\n",
        "df_cleaned.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"delta_vehicle_cleaned\")\n"
      ],
      "metadata": {
        "id": "n3DPMtb7U3sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate total maintenance cost for each vehicle\n",
        "df_total_cost = df.groupBy(\"VehicleID\").agg(sum(\"ServiceCost\").alias(\"TotalMaintenanceCost\"))\n",
        "\n",
        "# Identify vehicles exceeding the mileage threshold\n",
        "df_exceeding_mileage = df.filter(df[\"Mileage\"] > 3000).select(\"VehicleID\", \"Mileage\").distinct()\n",
        "\n",
        "# Join total cost and exceeding mileage data\n",
        "df_analysis = df_total_cost.join(df_exceeding_mileage, on=\"VehicleID\", how=\"left\")\n",
        "\n",
        "# Save the analysis results to a new Delta table\n",
        "df_analysis.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"delta_vehicle_analysis\")"
      ],
      "metadata": {
        "id": "Vo-DBCtWWTtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"optimize delta_vehicle_cleaned\")\n",
        "spark.sql(\"vacuum delta_vehicle_cleaned retain 168 hours\")"
      ],
      "metadata": {
        "id": "16F8PU-UXCVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task-2**"
      ],
      "metadata": {
        "id": "KMLL5pimXPf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dbutils.fs.cp(\"file:/content/ratings.csv\",\"dbfs/filestore/ratings.csv\")\n",
        "ratings_df=spark.read.format(\"csv\").option(\"header\",\"true\").load(\"dbfs:/filestore/ratings.csv\")\n",
        "ratings_df.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/filestore/delta_ratings\")\n",
        "\n",
        "try:\n",
        "  ratings_df=spark.read.format(\"delta\").load(\"dbfs:/filestore/delta_ratings\")\n",
        "except:\n",
        "    print(\"An error Occured\")\n",
        "\n",
        "# registering as a table\n",
        "%sql\n",
        "create table delta_ratings using delta location \"dbfs:/filestore/delta_ratings\"\n"
      ],
      "metadata": {
        "id": "5xIZH44GXSN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cleaning table\n",
        "df=spark.read.table(\"delta_ratings\")\n",
        "df_cleaned=df.filter((df[\"rating\"]>=1) & (df[\"rating\"]<=5))\n",
        "df_cleaned.removeDuplicates([\"userId\",\"movieId\"])\n",
        "\n",
        "# writing into a new delta file\n",
        "df_cleaned.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"delta_ratings_cleaned\")"
      ],
      "metadata": {
        "id": "YRcWzFfxZcBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_aggregated=df.groupBy(\"movieId\").agg(avg(\"rating\").alias(\"average_rating\"))\n",
        "df_highest=df.groupBy(\"movieId\").agg(max(\"rating\").alias(\"highest_rating\")).select(\"movieId\",\"highest_rating\")\n",
        "df_lowest=df.groupBy(\"movieId\").agg(min(\"rating\").alias(\"lowest_rating\")).select(\"movieId\",\"lowest_rating\")\n",
        "df_aggregated=df_aggregated.join(df_highest,on=\"movieId\",how=\"left\").join(df_lowest,on=\"movieId\",how=\"left\")\n",
        "df_aggregated.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"delta_ratings_aggregated\")"
      ],
      "metadata": {
        "id": "xIxtUGxJZy9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('''\n",
        "update delta_ratings_cleaned\n",
        "set 'rating'=4.5\n",
        "where 'rating'=5\n",
        "''')\n",
        "df_previous=spark.read.format(\"delta\").option(\"versionAsOf\",\"0\").load(\"dbfs:/filestore/delta_ratings_cleaned\")\n",
        "df_previous.show()\n",
        "\n",
        "spark.sql(\"Describe History delta_ratings_cleaned\")\n",
        "\n",
        "spark.sql(\"optimize delta_ratings_cleaned zorder by(\"movieID\")\")\n",
        "spark.sql(\"vacuum delta_ratings_cleaned retain 168 hours\")"
      ],
      "metadata": {
        "id": "hOvU0m_vaSFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**task-3**"
      ],
      "metadata": {
        "id": "MA4xS0D1f--F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dbutils.fs.cp(\"file:/content/student.csv\",\"dbfs:/filestore/student.csv\")\n",
        "dbutils.fs.cp(\"file:/content/city.csv\",\"dbfs:/filestore/city.csv\")\n",
        "df_student=spark.read.format(\"csv\").option(\"header\",\"true\").load(\"dbfs:/filestore/student.csv\")\n",
        "df_city=spark.read.format(\"json\").option(\"header\",\"true\").load(\"dbfs:/filestore/city.csv\")\n",
        "hospital_data = {\n",
        "    \"HospitalID\": [101, 102, 103, 104],\n",
        "    \"HospitalName\": [\"City Hospital\", \"Green Valley Clinic\", \"Sunshine Medical\", \"Downtown Health Center\"],\n",
        "    \"City\": [\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\"],\n",
        "    \"Capacity\": [250, 100, 300, 200],\n",
        "    \"EmergencyServices\": [True, False, True, True]\n",
        "}\n",
        "df_hospital = pd.DataFrame(hospital_data)\n",
        "df_hospital.to_parquet(\"hospital_data.parquet\")\n",
        "df_student.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/filestore/delta_student\")\n",
        "df_city.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/filestore/delta_city\")\n",
        "df_hospital.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/filestore/delta_hospital\")\n",
        "\n",
        "%sql\n",
        "create table delta_student using delta location \"dbfs:/filestore/delta_student\"\n",
        "\n",
        "%sql\n",
        "create table delta_city using delta location \"dbfs:/filestore/delta_city\"\n",
        "\n",
        "%sql\n",
        "create table delta_hospital using delta location \"dbfs:/filestore/delta_hospital\"\n",
        "\n",
        "df=spark.read.table(\"delta_hospital\")\n",
        "hospital_cleaned=df.removeDuplicates()\n",
        "hospital_cleaned.dropna(inplace=True)\n",
        "hospital_cleaned.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"delta_hospital_cleaned\")\n",
        "\n",
        "df=spark.read.table(\"delta_student\")\n",
        "student_cleaned=df.removeDuplicates()\n",
        "student_cleaned.dropna(inplace=True)\n",
        "student_cleaned.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"delta_student_cleaned\")\n",
        "\n",
        "df=spark.read.table(\"delta_city\")\n",
        "city_cleaned=df.removeDuplicates()\n",
        "city_cleaned.dropna(inplace=True)\n",
        "city_cleaned.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"delta_city_cleaned\")\n",
        "\n",
        "spark.notebook.run(\"https://colab.research.google.com/drive/1HKA7y2u2u1ZGZGVm-Oli70LWYHv9R35i#scrollTo=n1PSDRg7jnt5\",80)"
      ],
      "metadata": {
        "id": "VKDfZNBDf-Vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Additional commands\n",
        "spark.sql(\"optimize delta_student_cleaned\")\n",
        "spark.sql(\"vacuum delta_student_cleaned retain 168 hours\")"
      ],
      "metadata": {
        "id": "D2N1d0xxkYMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK-4**"
      ],
      "metadata": {
        "id": "0mz1A-KnkY0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dbutils.fs.cp(\"file:/content/transactions.csv\",\"dbfs:/filestore/transactions.csv\")\n",
        "df=spark.read.format(\"csv\").option(\"header\",\"true\").load(\"dbfs:/filestore/transactions.csv\")\n",
        "df.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/filestore/delta_transactions\")\n",
        "delta_transactions=spark.read.format(\"delta\").load(\"dbfs:/filestore/delta_transactions\")\n",
        "#Adding new aggreagte column\n",
        "delta_transactions=delta_transactions.withColumn(\"total_amount\",col(\"quantity\")*col(\"price\"))\n",
        "delta_transactions.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"delta_transactions\")\n"
      ],
      "metadata": {
        "id": "eOTSqfwXkkHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import DLT"
      ],
      "metadata": {
        "id": "gVP6BlzJlwf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dlt\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "@dlt.table\n",
        "def raw_transactions():\n",
        "    return spark.read.csv(\"dbfs:/filestore/transactions.csv\", header=True, inferSchema=True)\n",
        "\n",
        "@dlt.table\n",
        "def transformed_transactions():\n",
        "    return (\n",
        "        dlt.read(\"raw_transactions\")\n",
        "        .withColumn(\"total_amount\", col(\"Quantity\") * col(\"Price\"))\n",
        "    )\n",
        "spark.sql('''\n",
        "CREATE OR REFRESH LIVE TABLE raw_transactions AS\n",
        "SELECT * FROM read_csv(\"path_to_csv_file/transactions.csv\");\n",
        "''')\n",
        "\n",
        "spark.sql('''\n",
        "CREATE OR REFRESH LIVE TABLE transformed_transactions AS\n",
        "SELECT *,(Quantity * Price) AS total_amount\n",
        "FROM live.raw_transactions;\n",
        "''')\n"
      ],
      "metadata": {
        "id": "aDIZaDGult_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.read.table(\"delta_transactions\").show()\n",
        "\n",
        "dbutils.fs.cp(\"file:/content/new_transactions.csv\",\"dbfs:/filestore/new_transactions.csv\")\n",
        "new_transactions=spark.read.format(\"csv\").option(\"header\",\"true\").load(\"dbfs:/filestore/new_transactions.csv\")\n",
        "new_transactions=new_transactions.withColumn(\"total_amount\",col(\"quantity\")*col(\"price\"))\n",
        "#merging data into old table\n",
        "new_transactions.write.format(\"delta\").mode(\"append\").saveAsTable(\"delta_transactions\")\n",
        "\n",
        "#registering table\n",
        "%sql\n",
        "create table delta_transactions using delta location \"dbfs:/filestore/delta_transactions\"\n",
        "\n",
        "spark.sql('''\n",
        "update delta_transactions\n",
        "set 'price'=1300\n",
        "where product='Laptop'\n",
        "''')\n",
        "\n",
        "spark.sql('''\n",
        "delete from delta_transactions\n",
        "where \"Quantity\"<3\n",
        "''')\n",
        "\n",
        "%sql\n",
        "delete from delta_transactions where \"Quantity\"<3"
      ],
      "metadata": {
        "id": "vfHGZ4OZmY3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write new data to a temporary Delta table\n",
        "df_new_transactions=spark.read.format(\"csv\").option(\"header\",\"true\").load(\"dbfs:/filestore/new_transactions.csv\")\n",
        "df_new_transactions.write.format(\"delta\").mode(\"overwrite\").save(\"/filestore/delta/new_transactions\")\n",
        "spark.sql('''\n",
        "MERGE INTO transactions AS existing\n",
        "USING new_transactions AS updates\n",
        "ON existing.TransactionID = updates.TransactionID\n",
        "WHEN MATCHED THEN\n",
        "    UPDATE SET\n",
        "        existing.TransactionDate = updates.TransactionDate,\n",
        "        existing.CustomerID = updates.CustomerID,\n",
        "        existing.Product = updates.Product,\n",
        "        existing.Quantity = updates.Quantity,\n",
        "        existing.Price = updates.Price\n",
        "WHEN NOT MATCHED THEN\n",
        "    INSERT (TransactionID, TransactionDate, CustomerID, Product, Quantity, Price)\n",
        "    VALUES (updates.TransactionID, updates.TransactionDate, updates.CustomerID, updates.Product, updates.Quantity, updates.Price)\n",
        "''')"
      ],
      "metadata": {
        "id": "tSKGioHQoode"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"optimize delta_transactions\")\n",
        "spark.sql(\"vacuum delta_transactions retain 168 hours\")\n",
        "\n",
        "df=pd.read_csv(\"transactions.csv\")\n",
        "paraquet_transactions=df.to_parquet(\"paraquet_transactions.parquet\")\n",
        "dbutils.fs.cp(\"file:/content/paraquet_transactions.parquet\",\"dbfs:/filestore/paraquet_transactions.parquet\")\n",
        "\n",
        "transactions_pqt=spark.read.format(\"parquet\").load(\"dbfs:/filestore/paraquet_transactions.parquet\")\n"
      ],
      "metadata": {
        "id": "pH2XSy8dpJEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new_incremental = df_new_transactions.filter(\"TransactionDate > '2024-09-03'\")\n",
        "\n",
        "# Append new transactions to the existing Delta table\n",
        "df_new_incremental.write.format(\"delta\").mode(\"append\").save(\"path_to_delta_table/transactions\")\n",
        "delta_table = DeltaTable.forPath(spark, \"path_to_delta_table/transactions\")\n",
        "\n",
        "spark.sql(\"describe history df_new_incremental\")\n",
        "\n",
        "# You can also check specific versions for data\n",
        "# Check the latest version of the Delta table\n",
        "latest_version = delta_table.history().first().version\n",
        "print(f\"Latest version: {latest_version}\")\n",
        "\n",
        "# Optionally, read the Delta table for verification\n",
        "df_transactions = spark.read.format(\"delta\").load(\"path_to_delta_table/transactions\")\n",
        "df_transactions.show()"
      ],
      "metadata": {
        "id": "3nRinbydp2SC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}