{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4011,"status":"ok","timestamp":1725959121715,"user":{"displayName":"Satya Sai Durga Pavan Namana","userId":"10810514506066596976"},"user_tz":-330},"id":"KryWBeCnKTx0","outputId":"d55578d7-1573-4b71-b9b6-32d6a9577ed9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.2)\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"]}],"source":["!pip install pyspark"]},{"cell_type":"markdown","metadata":{"id":"8AJX5QFsKWEm"},"source":["# **Dataset: E-commerce Transaction**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":713,"status":"ok","timestamp":1725959122424,"user":{"displayName":"Satya Sai Durga Pavan Namana","userId":"10810514506066596976"},"user_tz":-330},"id":"TCONBmKbKYil","outputId":"22df5f7a-73b4-45d3-bbd4-71af2689673d"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+\n","|transaction_id|customer_id|     product|      category|price|quantity|discount_percentage|transaction_date|\n","+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+\n","|             1|        101|      Laptop|   Electronics| 1000|       1|                 10|      2023-08-01|\n","|             2|        102|  Smartphone|   Electronics|  700|       2|                  5|      2023-08-01|\n","|             3|        103|       Shirt|       Fashion|   40|       3|                  0|      2023-08-02|\n","|             4|        104|     Blender|Home Appliance|  150|       1|                 15|      2023-08-03|\n","|             5|        101|  Headphones|   Electronics|  100|       2|                 10|      2023-08-03|\n","|             6|        105|       Shoes|       Fashion|   60|       1|                 20|      2023-08-04|\n","|             7|        106|Refrigerator|Home Appliance|  800|       1|                 25|      2023-08-05|\n","|             8|        107|        Book|         Books|   20|       4|                  0|      2023-08-05|\n","|             9|        108|     Toaster|Home Appliance|   30|       1|                  5|      2023-08-06|\n","|            10|        102|      Tablet|   Electronics|  300|       2|                 10|      2023-08-06|\n","+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+\n","\n","root\n"," |-- transaction_id: long (nullable = true)\n"," |-- customer_id: long (nullable = true)\n"," |-- product: string (nullable = true)\n"," |-- category: string (nullable = true)\n"," |-- price: long (nullable = true)\n"," |-- quantity: long (nullable = true)\n"," |-- discount_percentage: long (nullable = true)\n"," |-- transaction_date: string (nullable = true)\n","\n"]}],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import *\n","\n","spark=SparkSession.builder.appName(\"Ecomm data\").getOrCreate()\n","\n","transactions=[(1,101,'Laptop','Electronics',1000,1,10,'2023-08-01'),\n","(2,102,'Smartphone','Electronics',700,2,5,'2023-08-01'),\n","(3,103,'Shirt','Fashion',40,3,0,'2023-08-02'),\n","(4,104,'Blender','Home Appliance',150,1,15,'2023-08-03'),\n","(5,101,'Headphones','Electronics',100,2,10,'2023-08-03'),\n","(6,105,'Shoes','Fashion',60,1,20,'2023-08-04'),\n","(7,106,'Refrigerator','Home Appliance',800,1,25,'2023-08-05'),\n","(8,107,'Book','Books',20,4,0,'2023-08-05'),\n","(9,108,'Toaster','Home Appliance',30,1,5,'2023-08-06'),\n","(10,102,'Tablet','Electronics',300,2,10,'2023-08-06'),\n","]\n","\n","transaction_columns=['transaction_id','customer_id','product','category','price','quantity','discount_percentage','transaction_date']\n","transactions_df=spark.createDataFrame(transactions,schema=transaction_columns)\n","transactions_df.show()\n","transactions_df.printSchema()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9181,"status":"ok","timestamp":1725959131600,"user":{"displayName":"Satya Sai Durga Pavan Namana","userId":"10810514506066596976"},"user_tz":-330},"id":"gwLRsCIxKaTf","outputId":"f7acc839-4c70-451c-f430-ad905cf2dcfe"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------+------------+\n","|      category|sum(revenue)|\n","+--------------+------------+\n","|       Fashion|      9988.0|\n","|   Electronics|    209825.0|\n","|Home Appliance|     97776.0|\n","|         Books|      2000.0|\n","+--------------+------------+\n","\n","+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+\n","|transaction_id|customer_id|     product|      category|price|quantity|discount_percentage|transaction_date|\n","+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+\n","|             4|        104|     Blender|Home Appliance|  150|       1|                 15|      2023-08-03|\n","|             6|        105|       Shoes|       Fashion|   60|       1|                 20|      2023-08-04|\n","|             7|        106|Refrigerator|Home Appliance|  800|       1|                 25|      2023-08-05|\n","+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+\n","\n","+--------------+-----------+-------+-----------+-----+--------+-------------------+----------------+\n","|transaction_id|customer_id|product|   category|price|quantity|discount_percentage|transaction_date|\n","+--------------+-----------+-------+-----------+-----+--------+-------------------+----------------+\n","|             1|        101| Laptop|Electronics| 1000|       1|                 10|      2023-08-01|\n","+--------------+-----------+-------+-----------+-----+--------+-------------------+----------------+\n","\n","+--------------+-----------------+\n","|      category|       avg(price)|\n","+--------------+-----------------+\n","|       Fashion|             50.0|\n","|   Electronics|            525.0|\n","|Home Appliance|326.6666666666667|\n","|         Books|             20.0|\n","+--------------+-----------------+\n","\n","+-----------+-----+\n","|customer_id|count|\n","+-----------+-----+\n","|        101|    2|\n","|        102|    2|\n","+-----------+-----+\n","\n","+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+-------+\n","|transaction_id|customer_id|     product|      category|price|quantity|discount_percentage|transaction_date|revenue|\n","+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+-------+\n","|             2|        102|  Smartphone|   Electronics|  700|       2|                  5|      2023-08-01|   1400|\n","|             1|        101|      Laptop|   Electronics| 1000|       1|                 10|      2023-08-01|   1000|\n","|             7|        106|Refrigerator|Home Appliance|  800|       1|                 25|      2023-08-05|    800|\n","+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+-------+\n","\n","+----------------+-----+\n","|transaction_date|count|\n","+----------------+-----+\n","|      2023-08-01|    2|\n","|      2023-08-02|    1|\n","|      2023-08-03|    2|\n","|      2023-08-06|    2|\n","|      2023-08-04|    1|\n","|      2023-08-05|    2|\n","+----------------+-----+\n","\n","+-----------+----------------+\n","|Customer_id|customer_revenue|\n","+-----------+----------------+\n","|        102|            2000|\n","+-----------+----------------+\n","\n","+--------------+------------------------+\n","|      category|avg(discount_percentage)|\n","+--------------+------------------------+\n","|       Fashion|                    10.0|\n","|   Electronics|                    8.75|\n","|Home Appliance|                    15.0|\n","|         Books|                     0.0|\n","+--------------+------------------------+\n","\n","+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+-----------+\n","|transaction_id|customer_id|     product|      category|price|quantity|discount_percentage|transaction_date|final_price|\n","+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+-----------+\n","|             1|        101|      Laptop|   Electronics| 1000|       1|                 10|      2023-08-01|      900.0|\n","|             2|        102|  Smartphone|   Electronics|  700|       2|                  5|      2023-08-01|      665.0|\n","|             3|        103|       Shirt|       Fashion|   40|       3|                  0|      2023-08-02|       40.0|\n","|             4|        104|     Blender|Home Appliance|  150|       1|                 15|      2023-08-03|      127.5|\n","|             5|        101|  Headphones|   Electronics|  100|       2|                 10|      2023-08-03|       90.0|\n","|             6|        105|       Shoes|       Fashion|   60|       1|                 20|      2023-08-04|       48.0|\n","|             7|        106|Refrigerator|Home Appliance|  800|       1|                 25|      2023-08-05|      600.0|\n","|             8|        107|        Book|         Books|   20|       4|                  0|      2023-08-05|       20.0|\n","|             9|        108|     Toaster|Home Appliance|   30|       1|                  5|      2023-08-06|       28.5|\n","|            10|        102|      Tablet|   Electronics|  300|       2|                 10|      2023-08-06|      270.0|\n","+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+-----------+\n","\n"]}],"source":["\n","#1. Calculate the Total Revenue per Category\n","\n","Total_revenue=transactions_df.withColumn(\"revenue\",(100-(col(\"discount_percentage\")/100))*col(\"price\")).groupBy(\"category\").sum(\"revenue\")\n","Total_revenue.show()\n","\n","#2. Filter Transactions with a Discount Greater Than 10%\n","\n","filter1=transactions_df.filter(transactions_df.discount_percentage>10)\n","filter1.show()\n","\n","#3. Find the Most Expensive Product Sold\n","\n","max_priced_product=transactions_df.orderBy(desc(\"price\")).limit(1)\n","max_priced_product.show()\n","\n","# 4.average price of products per category\n","\n","average_price=transactions_df.groupBy(\"category\").avg(\"price\")\n","average_price.show()\n","\n","# 5.Who bougth more than one product\n","multiple_transactions=transactions_df.groupBy(\"customer_id\").count().filter(col(\"count\")>1)\n","multiple_transactions.show()\n","\n","#6. Find the Top 3 Highest Revenue Transactions\n","Total_revenue=transactions_df.withColumn(\"revenue\",col(\"price\")*col(\"quantity\"))\n","top_three=Total_revenue.orderBy(desc(\"revenue\")).limit(3)\n","top_three.show()\n","\n","#7. Calculate the Total Number of Transactions per Day\n","total_transactions_per_day=transactions_df.groupBy(\"transaction_date\").count()\n","total_transactions_per_day.show()\n","\n","#8. Customer Who spent most money\n","revenue=transactions_df.withColumn(\"revenue\",col(\"quantity\")*col(\"price\"))\n","Valuable_customer_revenue=revenue.groupBy(\"Customer_id\").agg(sum(\"revenue\").alias(\"customer_revenue\"))\n","Valuable_customer=Valuable_customer_revenue.orderBy(desc(\"customer_revenue\")).limit(1)\n","Valuable_customer.show()\n","\n","#9. Calculate the Average Discount Given per Product Category\n","discount_products=transactions_df.groupBy(\"category\").avg(\"discount_percentage\")\n","discount_products.show()\n","\n","#10. Create a New Column for Final Price After Discount\n","final_price=transactions_df.withColumn(\"final_price\", ( col(\"price\") - (col('price') * col('discount_percentage') / 100) ))\n","final_price.show()"]},{"cell_type":"markdown","metadata":{"id":"fhYqBSq4Kf7d"},"source":["# **DataSet: Banking Transactions**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1556,"status":"ok","timestamp":1725959133153,"user":{"displayName":"Satya Sai Durga Pavan Namana","userId":"10810514506066596976"},"user_tz":-330},"id":"EsQNdXYUKiJs","outputId":"a1b4ef59-683d-4fa0-fe96-60785d251629"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------+-----------+----------------+------+----------------+\n","|transaction_id|customer_id|transaction_type|amount|transaction_date|\n","+--------------+-----------+----------------+------+----------------+\n","|             1|        201|         Deposit|  5000|      2023-09-01|\n","|             2|        202|      Withdrawal|  2000|      2023-09-01|\n","|             3|        203|         Deposit|  3000|      2023-09-02|\n","|             4|        201|      Withdrawal|  1500|      2023-09-02|\n","|             5|        204|         Deposit| 10000|      2023-09-03|\n","|             6|        205|      Withdrawal|   500|      2023-09-03|\n","|             7|        202|         Deposit|  2500|      2023-09-04|\n","|             8|        206|      Withdrawal|   700|      2023-09-04|\n","|             9|        203|         Deposit|  4000|      2023-09-05|\n","|            10|        204|      Withdrawal|  3000|      2023-09-05|\n","+--------------+-----------+----------------+------+----------------+\n","\n","root\n"," |-- transaction_id: long (nullable = true)\n"," |-- customer_id: long (nullable = true)\n"," |-- transaction_type: string (nullable = true)\n"," |-- amount: long (nullable = true)\n"," |-- transaction_date: string (nullable = true)\n","\n"]}],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql import *\n","\n","spark=SparkSession.builder.appName(\"banking data\").getOrCreate()\n","data = [\n","    (1, 201, \"Deposit\", 5000, \"2023-09-01\"),\n","    (2, 202, \"Withdrawal\", 2000, \"2023-09-01\"),\n","    (3, 203, \"Deposit\", 3000, \"2023-09-02\"),\n","    (4, 201, \"Withdrawal\", 1500, \"2023-09-02\"),\n","    (5, 204, \"Deposit\", 10000, \"2023-09-03\"),\n","    (6, 205, \"Withdrawal\", 500, \"2023-09-03\"),\n","    (7, 202, \"Deposit\", 2500, \"2023-09-04\"),\n","    (8, 206, \"Withdrawal\", 700, \"2023-09-04\"),\n","    (9, 203, \"Deposit\", 4000, \"2023-09-05\"),\n","    (10, 204, \"Withdrawal\", 3000, \"2023-09-05\")\n","]\n","columns1=[\"transaction_id\",\"customer_id\",\"transaction_type\",\"amount\",\"transaction_date\"]\n","banking_df=spark.createDataFrame(data,schema=columns1)\n","banking_df.show()\n","banking_df.printSchema()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7797,"status":"ok","timestamp":1725959140947,"user":{"displayName":"Satya Sai Durga Pavan Namana","userId":"10810514506066596976"},"user_tz":-330},"id":"IDWr9gdEKjGZ","outputId":"5a26832f-0b7a-4d7c-e904-fd32bbed74a4"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------------+-----------+\n","|transaction_type|sum(amount)|\n","+----------------+-----------+\n","|         Deposit|      24500|\n","|      Withdrawal|       7700|\n","+----------------+-----------+\n","\n","+--------------+-----------+----------------+------+----------------+\n","|transaction_id|customer_id|transaction_type|amount|transaction_date|\n","+--------------+-----------+----------------+------+----------------+\n","|             1|        201|         Deposit|  5000|      2023-09-01|\n","|             5|        204|         Deposit| 10000|      2023-09-03|\n","|             9|        203|         Deposit|  4000|      2023-09-05|\n","+--------------+-----------+----------------+------+----------------+\n","\n","+--------------+-----------+----------------+------+----------------+\n","|transaction_id|customer_id|transaction_type|amount|transaction_date|\n","+--------------+-----------+----------------+------+----------------+\n","|             5|        204|         Deposit| 10000|      2023-09-03|\n","+--------------+-----------+----------------+------+----------------+\n","\n","+----------------+-----------+\n","|transaction_type|avg(amount)|\n","+----------------+-----------+\n","|         Deposit|     4900.0|\n","|      Withdrawal|     1540.0|\n","+----------------+-----------+\n","\n","+-----------+\n","|customer_id|\n","+-----------+\n","|        202|\n","|        201|\n","|        204|\n","+-----------+\n","\n","+----------------+-----------+\n","|transaction_date|sum(amount)|\n","+----------------+-----------+\n","|      2023-09-01|       7000|\n","|      2023-09-02|       4500|\n","|      2023-09-03|      10500|\n","|      2023-09-05|       7000|\n","|      2023-09-04|       3200|\n","+----------------+-----------+\n","\n","+-----------+-----------+\n","|customer_id|sum(amount)|\n","+-----------+-----------+\n","|        204|       3000|\n","+-----------+-----------+\n","\n","+-----------+-----+\n","|customer_id|count|\n","+-----------+-----+\n","|        202|    2|\n","|        201|    2|\n","|        203|    2|\n","|        204|    2|\n","|        205|    1|\n","|        206|    1|\n","+-----------+-----+\n","\n","+----------------+--------------+-----------+----------------+------+\n","|transaction_date|transaction_id|customer_id|transaction_type|amount|\n","+----------------+--------------+-----------+----------------+------+\n","|      2023-09-01|             1|        201|         Deposit|  5000|\n","|      2023-09-01|             2|        202|      Withdrawal|  2000|\n","|      2023-09-02|             3|        203|         Deposit|  3000|\n","|      2023-09-02|             4|        201|      Withdrawal|  1500|\n","|      2023-09-05|             9|        203|         Deposit|  4000|\n","|      2023-09-05|            10|        204|      Withdrawal|  3000|\n","+----------------+--------------+-----------+----------------+------+\n","\n","+--------------+-----------+----------------+------+----------------+-----------------+\n","|transaction_id|customer_id|transaction_type|amount|transaction_date|transaction_value|\n","+--------------+-----------+----------------+------+----------------+-----------------+\n","|             1|        201|         Deposit|  5000|      2023-09-01|             High|\n","|             2|        202|      Withdrawal|  2000|      2023-09-01|              Low|\n","|             3|        203|         Deposit|  3000|      2023-09-02|              Low|\n","|             4|        201|      Withdrawal|  1500|      2023-09-02|              Low|\n","|             5|        204|         Deposit| 10000|      2023-09-03|             High|\n","|             6|        205|      Withdrawal|   500|      2023-09-03|              Low|\n","|             7|        202|         Deposit|  2500|      2023-09-04|              Low|\n","|             8|        206|      Withdrawal|   700|      2023-09-04|              Low|\n","|             9|        203|         Deposit|  4000|      2023-09-05|              Low|\n","|            10|        204|      Withdrawal|  3000|      2023-09-05|              Low|\n","+--------------+-----------+----------------+------+----------------+-----------------+\n","\n"]}],"source":["#1. Calculate the Total Deposit and Withdrawal Amounts\n","\n","total_grouped_data=banking_df.groupBy(\"transaction_type\").sum(\"amount\")\n","total_grouped_data.show()\n","\n","#2. Filter Transactions Greater Than $3,000\n","filter2=banking_df.filter(banking_df.amount>3000)\n","filter2.show()\n","\n","#3. Find the Largest Deposit Made\n","largest_deposit=banking_df.filter(banking_df.transaction_type==\"Deposit\").orderBy(desc(\"amount\")).limit(1)\n","largest_deposit.show()\n","\n","#4. Calculate the Average Withdrawal Amount\n","average_withdrawal=banking_df.groupBy(\"transaction_type\").agg(avg(\"amount\"))\n","average_withdrawal.show()\n","\n","#5. Find Customers Who Made Both Deposits and Withdrawals\n","deposit_df = banking_df.filter(col(\"transaction_type\") == \"Deposit\").select(\"customer_id\").distinct()\n","withdrawal_df = banking_df.filter(col(\"transaction_type\") == \"Withdrawal\").select(\"customer_id\").distinct()\n","customers_both = deposit_df.join(withdrawal_df, on=\"customer_id\", how=\"inner\")\n","customers_both.show()\n","\n","# 6. Calculate the Total Amount of Transactions per Day\n","transactions_per_day=banking_df.groupBy(\"transaction_date\").sum(\"amount\")\n","transactions_per_day.show()\n","\n","#7. Find the Customer with the Highest Total Withdrawal\n","highest_withdrawl=banking_df.filter(banking_df.transaction_type==\"Withdrawal\").groupBy(\"customer_id\").sum(\"amount\").orderBy(desc(\"sum(amount)\")).limit(1)\n","highest_withdrawl.show()\n","\n","# 8. Calculate the Number of Transactions for Each Customer\n","transactions_per_customer=banking_df.groupBy(\"customer_id\").count()\n","transactions_per_customer.show()\n","\n","#9.Find All Transactions That Occurred on the Same Day as a Withdrawal Greater Than $1,000\n","withdrawals_df = banking_df.filter((col(\"transaction_type\") == \"Withdrawal\") & (col(\"amount\") > 1000))\n","dates_with_large_withdrawals = withdrawals_df.select(\"transaction_date\").distinct()\n","all_transactions_on_dates = banking_df.join(dates_with_large_withdrawals, on=\"transaction_date\", how=\"inner\")\n","all_transactions_on_dates.show()\n","\n","#10. Create a New Column to Classify Transactions as \"High\" or \"Low\" Value\n","classifier=banking_df.withColumn(\"transaction_value\", when(col(\"amount\")>=5000,\"High\").otherwise(\"Low\"))\n","classifier.show()"]},{"cell_type":"markdown","metadata":{"id":"CPp7_eZ_KlRG"},"source":["# **Dataset: Health & Fitness Tracker Data**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":405,"status":"ok","timestamp":1725959141336,"user":{"displayName":"Satya Sai Durga Pavan Namana","userId":"10810514506066596976"},"user_tz":-330},"id":"DoSc5790Kq84","outputId":"54e2cf8e-557b-42a1-88e8-e10b95643627"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+----------+-----------+---------------+--------------+------------+\n","|user_id|      date|steps_count|calories_burned|hours_of_sleep|workout_type|\n","+-------+----------+-----------+---------------+--------------+------------+\n","|      1|2023-09-01|      12000|            500|           7.0|      Cardio|\n","|      2|2023-09-01|       8000|            400|           6.5|    Strength|\n","|      3|2023-09-01|      15000|            650|           8.0|        Yoga|\n","|      1|2023-09-02|      10000|            450|           6.0|      Cardio|\n","|      2|2023-09-02|       9500|            500|           7.0|      Cardio|\n","|      3|2023-09-02|      14000|            600|           7.5|    Strength|\n","|      1|2023-09-03|      13000|            550|           8.0|        Yoga|\n","|      2|2023-09-03|      12000|            520|           6.5|        Yoga|\n","|      3|2023-09-03|      16000|            700|           7.0|      Cardio|\n","+-------+----------+-----------+---------------+--------------+------------+\n","\n","root\n"," |-- user_id: integer (nullable = true)\n"," |-- date: string (nullable = true)\n"," |-- steps_count: integer (nullable = true)\n"," |-- calories_burned: integer (nullable = true)\n"," |-- hours_of_sleep: double (nullable = true)\n"," |-- workout_type: string (nullable = true)\n","\n"]}],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType\n","from pyspark.sql.functions import col\n","\n","spark = SparkSession.builder.appName(\"Health Data\").getOrCreate()\n","data1 = [\n","    (1, \"2023-09-01\", 12000, 500, 7.0, \"Cardio\"),\n","    (2, \"2023-09-01\", 8000, 400, 6.5, \"Strength\"),\n","    (3, \"2023-09-01\", 15000, 650, 8.0, \"Yoga\"),\n","    (1, \"2023-09-02\", 10000, 450, 6.0, \"Cardio\"),\n","    (2, \"2023-09-02\", 9500, 500, 7.0, \"Cardio\"),\n","    (3, \"2023-09-02\", 14000, 600, 7.5, \"Strength\"),\n","    (1, \"2023-09-03\", 13000, 550, 8.0, \"Yoga\"),\n","    (2, \"2023-09-03\", 12000, 520, 6.5, \"Yoga\"),\n","    (3, \"2023-09-03\", 16000, 700, 7.0, \"Cardio\")\n","]\n","schema = StructType([\n","    StructField(\"user_id\", IntegerType(), True),\n","    StructField(\"date\", StringType(), True),\n","    StructField(\"steps_count\", IntegerType(), True),\n","    StructField(\"calories_burned\", IntegerType(), True),\n","    StructField(\"hours_of_sleep\", DoubleType(), True),\n","    StructField(\"workout_type\", StringType(), True)\n","])\n","health_df = spark.createDataFrame(data1, schema)\n","health_df.show()\n","health_df.printSchema()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kTf_c8zsKthJ","outputId":"0e27bd45-1a75-4c98-93bd-2970d93bac28"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+----------------+\n","|user_id|sum(steps_count)|\n","+-------+----------------+\n","|      1|           35000|\n","|      3|           45000|\n","|      2|           29500|\n","+-------+----------------+\n","\n","+-------+----------+-----------+---------------+--------------+------------+\n","|user_id|      date|steps_count|calories_burned|hours_of_sleep|workout_type|\n","+-------+----------+-----------+---------------+--------------+------------+\n","|      1|2023-09-01|      12000|            500|           7.0|      Cardio|\n","|      3|2023-09-01|      15000|            650|           8.0|        Yoga|\n","|      3|2023-09-02|      14000|            600|           7.5|    Strength|\n","|      1|2023-09-03|      13000|            550|           8.0|        Yoga|\n","|      2|2023-09-03|      12000|            520|           6.5|        Yoga|\n","|      3|2023-09-03|      16000|            700|           7.0|      Cardio|\n","+-------+----------+-----------+---------------+--------------+------------+\n","\n","+------------+--------------------+\n","|workout_type|avg(calories_burned)|\n","+------------+--------------------+\n","|    Strength|               500.0|\n","|        Yoga|   573.3333333333334|\n","|      Cardio|               537.5|\n","+------------+--------------------+\n","\n","+-------+----------+----------------+\n","|user_id|      date|sum(steps_count)|\n","+-------+----------+----------------+\n","|      3|2023-09-03|           16000|\n","|      3|2023-09-01|           15000|\n","|      3|2023-09-02|           14000|\n","|      1|2023-09-03|           13000|\n","|      1|2023-09-01|           12000|\n","|      2|2023-09-03|           12000|\n","|      1|2023-09-02|           10000|\n","|      2|2023-09-02|            9500|\n","|      2|2023-09-01|            8000|\n","+-------+----------+----------------+\n","\n","+-------+----------+-----------+---------------+--------------+------------+\n","|user_id|      date|steps_count|calories_burned|hours_of_sleep|workout_type|\n","+-------+----------+-----------+---------------+--------------+------------+\n","|      3|2023-09-01|      15000|            650|           8.0|        Yoga|\n","|      3|2023-09-03|      16000|            700|           7.0|      Cardio|\n","+-------+----------+-----------+---------------+--------------+------------+\n","\n"]}],"source":["\n","#1.Total steps by each user\n","total_steps=health_df.groupBy(\"user_id\").sum(\"steps_count\")\n","total_steps.show()\n","\n","# 2.Filter Days with More Than 10,000 Steps\n","filter3=health_df.filter(health_df.steps_count>10000)\n","filter3.show()\n","\n","#3.Average Calories Burned by Workout Type\n","average_calories=health_df.groupBy(\"workout_type\").avg(\"calories_burned\")\n","average_calories.show()\n","\n","# 4.Identify the Day with the Most Steps for Each User\n","most_steps=health_df.groupBy(\"user_id\",\"date\").sum(\"steps_count\").orderBy(\"sum(steps_count)\",ascending=False)\n","most_steps.show()\n","\n","#5.Find Users Who Burned More Than 600 Calories on Any Day\n","calories_burned=health_df.filter(health_df.calories_burned>600)\n","calories_burned.show()\n","\n","#6.calculate the Average Hours of Sleep per User\n","average_sleep=health_df.groupBy(\"user_id\").avg(\"hours_of_sleep\")\n","average_sleep.show()\n","\n","# 7.7. Find the Total Calories Burned per Day\n","avg_calories_per_day=health_df.groupBy(\"date\").sum(\"calories_burned\")\n","avg_calories_per_day.show()\n","\n","#8.identify Users Who Did Different Types of Workouts\n","diff_users = health_df.groupBy(\"user_id\").agg(countDistinct(\"workout_type\").alias(\"distinct_workout_types\"))\n","diff_users = diff_users.filter(col(\"distinct_workout_types\") > 1)\n","diff_users.show()\n","\n","#9 Calculate the Total Number of Workouts per User\n","workouts_per_user=health_df.groupBy(\"user_id\").count()\n","workouts_per_user.show()\n","\n","#10.Create a New Column for \"Active\" Days\n","\n","Active_days=health_df.withColumn(\"Activity\", when(col(\"calories_burned\")>10000,\"Active\").otherwise(\"Inactive\"))\n","Active_days.show()"]},{"cell_type":"markdown","metadata":{"id":"uyROxOykKw_-"},"source":["# **Dataset: Music Streaming Data**"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"KTqpzgYgKyxz","executionInfo":{"status":"ok","timestamp":1725959239835,"user_tz":-330,"elapsed":1063,"user":{"displayName":"Satya Sai Durga Pavan Namana","userId":"10810514506066596976"}},"outputId":"b4696cec-a711-4752-c728-5bb0552bc94d","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["+-------+---------------+----------+----------------+-------------------+-----------+\n","|user_id|     song_title|    artist|duration_seconds|     streaming_time|   location|\n","+-------+---------------+----------+----------------+-------------------+-----------+\n","|      1|Blinding Lights|The Weeknd|             200|2023-09-01 08:15:00|   New York|\n","|      2|   Shape of You|Ed Sheeran|             240|2023-09-01 09:20:00|Los Angeles|\n","|      3|     Levitating|  Dua Lipa|             180|2023-09-01 10:30:00|     London|\n","|      1|        Starboy|The Weeknd|             220|2023-09-01 11:00:00|   New York|\n","|      2|        Perfect|Ed Sheeran|             250|2023-09-01 12:15:00|Los Angeles|\n","|      3|Don't Start Now|  Dua Lipa|             200|2023-09-02 08:10:00|     London|\n","|      1|Save Your Tears|The Weeknd|             210|2023-09-02 09:00:00|   New York|\n","|      2|    Galway Girl|Ed Sheeran|             190|2023-09-02 10:00:00|Los Angeles|\n","|      3|      New Rules|  Dua Lipa|             230|2023-09-02 11:00:00|     London|\n","+-------+---------------+----------+----------------+-------------------+-----------+\n","\n","root\n"," |-- user_id: long (nullable = true)\n"," |-- song_title: string (nullable = true)\n"," |-- artist: string (nullable = true)\n"," |-- duration_seconds: long (nullable = true)\n"," |-- streaming_time: string (nullable = true)\n"," |-- location: string (nullable = true)\n","\n"]}],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType\n","from pyspark.sql.functions import col\n","\n","\n","spark = SparkSession.builder.appName(\"music data\").getOrCreate()\n","data = [\n","    (1, \"Blinding Lights\", \"The Weeknd\", 200, \"2023-09-01 08:15:00\", \"New York\"),\n","    (2, \"Shape of You\", \"Ed Sheeran\", 240, \"2023-09-01 09:20:00\", \"Los Angeles\"),\n","    (3, \"Levitating\", \"Dua Lipa\", 180, \"2023-09-01 10:30:00\", \"London\"),\n","    (1, \"Starboy\", \"The Weeknd\", 220, \"2023-09-01 11:00:00\", \"New York\"),\n","    (2, \"Perfect\", \"Ed Sheeran\", 250, \"2023-09-01 12:15:00\", \"Los Angeles\"),\n","    (3, \"Don't Start Now\", \"Dua Lipa\", 200, \"2023-09-02 08:10:00\", \"London\"),\n","    (1, \"Save Your Tears\", \"The Weeknd\", 210, \"2023-09-02 09:00:00\", \"New York\"),\n","    (2, \"Galway Girl\", \"Ed Sheeran\", 190, \"2023-09-02 10:00:00\", \"Los Angeles\"),\n","    (3, \"New Rules\", \"Dua Lipa\", 230, \"2023-09-02 11:00:00\", \"London\")\n","]\n","columns=[\"user_id\",\"song_title\",\"artist\",\"duration_seconds\",\"streaming_time\",'location']\n","music_df=spark.createDataFrame(data,schema=columns)\n","music_df.show()\n","music_df.printSchema()"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"qKoj4KUjK1hM","executionInfo":{"status":"ok","timestamp":1725959253143,"user_tz":-330,"elapsed":5344,"user":{"displayName":"Satya Sai Durga Pavan Namana","userId":"10810514506066596976"}},"outputId":"262ed444-f72a-4ab3-e512-27bc21ee0763","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["+-------+---------------------+\n","|user_id|sum(duration_seconds)|\n","+-------+---------------------+\n","|      1|                  630|\n","|      3|                  610|\n","|      2|                  680|\n","+-------+---------------------+\n","\n","+-------+---------------+----------+----------------+-------------------+-----------+\n","|user_id|     song_title|    artist|duration_seconds|     streaming_time|   location|\n","+-------+---------------+----------+----------------+-------------------+-----------+\n","|      2|   Shape of You|Ed Sheeran|             240|2023-09-01 09:20:00|Los Angeles|\n","|      1|        Starboy|The Weeknd|             220|2023-09-01 11:00:00|   New York|\n","|      2|        Perfect|Ed Sheeran|             250|2023-09-01 12:15:00|Los Angeles|\n","|      1|Save Your Tears|The Weeknd|             210|2023-09-02 09:00:00|   New York|\n","|      3|      New Rules|  Dua Lipa|             230|2023-09-02 11:00:00|     London|\n","+-------+---------------+----------+----------------+-------------------+-----------+\n","\n","+--------+-----+\n","|  artist|count|\n","+--------+-----+\n","|Dua Lipa|    3|\n","+--------+-----+\n","\n","+-------+----------+----------+----------------+-------------------+-----------+\n","|user_id|song_title|    artist|duration_seconds|     streaming_time|   location|\n","+-------+----------+----------+----------------+-------------------+-----------+\n","|      2|   Perfect|Ed Sheeran|             250|2023-09-01 12:15:00|Los Angeles|\n","+-------+----------+----------+----------------+-------------------+-----------+\n","\n","+----------+---------------------+\n","|    artist|avg(duration_seconds)|\n","+----------+---------------------+\n","|  Dua Lipa|   203.33333333333334|\n","|Ed Sheeran|   226.66666666666666|\n","|The Weeknd|                210.0|\n","+----------+---------------------+\n","\n","+----------+---------------------+\n","|    Artist|avg(duration_seconds)|\n","+----------+---------------------+\n","|  Dua Lipa|   203.33333333333334|\n","|Ed Sheeran|   226.66666666666666|\n","|The Weeknd|                210.0|\n","+----------+---------------------+\n","\n","+-------+---------------+-----+\n","|user_id|     song_title|count|\n","+-------+---------------+-----+\n","|      1|Blinding Lights|    1|\n","|      3|     Levitating|    1|\n","|      2|   Shape of You|    1|\n","+-------+---------------+-----+\n","\n","+-------+----------------+\n","|user_id|distinct_artists|\n","+-------+----------------+\n","+-------+----------------+\n","\n","+-----------+-----+\n","|   location|count|\n","+-----------+-----+\n","|Los Angeles|    3|\n","|     London|    3|\n","|   New York|    3|\n","+-----------+-----+\n","\n","+-------+---------------+----------+----------------+-------------------+-----------+-----------+\n","|user_id|     song_title|    artist|duration_seconds|     streaming_time|   location|song_length|\n","+-------+---------------+----------+----------------+-------------------+-----------+-----------+\n","|      1|Blinding Lights|The Weeknd|             200|2023-09-01 08:15:00|   New York|       Long|\n","|      2|   Shape of You|Ed Sheeran|             240|2023-09-01 09:20:00|Los Angeles|       Long|\n","|      3|     Levitating|  Dua Lipa|             180|2023-09-01 10:30:00|     London|      Short|\n","|      1|        Starboy|The Weeknd|             220|2023-09-01 11:00:00|   New York|       Long|\n","|      2|        Perfect|Ed Sheeran|             250|2023-09-01 12:15:00|Los Angeles|       Long|\n","|      3|Don't Start Now|  Dua Lipa|             200|2023-09-02 08:10:00|     London|       Long|\n","|      1|Save Your Tears|The Weeknd|             210|2023-09-02 09:00:00|   New York|       Long|\n","|      2|    Galway Girl|Ed Sheeran|             190|2023-09-02 10:00:00|Los Angeles|      Short|\n","|      3|      New Rules|  Dua Lipa|             230|2023-09-02 11:00:00|     London|       Long|\n","+-------+---------------+----------+----------------+-------------------+-----------+-----------+\n","\n"]}],"source":["# 1. Calculate the Total Listening Time for Each Use\n","listening_time=music_df.groupBy(\"user_id\").sum(\"duration_seconds\")\n","listening_time.show()\n","\n","#2. Filter Songs Streamed for More Than 200 Seconds\n","streamed_songs=music_df.filter(music_df.duration_seconds>200)\n","streamed_songs.show()\n","\n","#3. Find the Most Popular Artist\n","most_popular_artist=music_df.groupBy(\"artist\").count().orderBy(desc(\"count\")).limit(1)\n","most_popular_artist.show()\n","\n","#4 . Identify the Song with the Longest Duration\n","longest_song=music_df.orderBy(desc(\"duration_seconds\")).limit(1)\n","longest_song.show()\n","\n","#5. Calculate the Average streaming time per artist\n","average_listening_time=music_df.groupBy(\"artist\").avg(\"duration_seconds\")\n","average_listening_time.show()\n","\n","#6.calculate the Average Song Duration by Artist\n","avg_song_duration=music_df.groupBy(\"Artist\").avg(\"duration_seconds\")\n","avg_song_duration.show()\n","\n","#7.Find the Top 3 Most Streamed Songs per User\n","top_streamed=music_df.groupBy(\"user_id\",\"song_title\").count().orderBy(desc(\"count\")).limit(3)\n","top_streamed.show()\n","\n","#8.Identify Users Who Streamed Songs from More Than One Artist\n","multiple_songs_listeners=music_df.groupBy(\"user_id\").agg(countDistinct(\"artist\").alias(\"distinct_artists\"))\n","multiple_songs_listeners=multiple_songs_listeners.filter(col(\"distinct_artists\")>1)\n","multiple_songs_listeners.show()\n","\n","#9. Calculate the Total Streams for Each Location\n","total_streams=music_df.groupBy(\"location\").count()\n","total_streams.show()\n","\n","#10. Create a New Column to Classify Long and Short Songs\n","classifier=music_df.withColumn(\"song_length\", when(col(\"duration_seconds\")>=200,\"Long\").otherwise(\"Short\"))\n","classifier.show()"]},{"cell_type":"markdown","metadata":{"id":"rVmz5iGtK5K_"},"source":["# **Dataset: Retail Store Sales Data**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"yXnRSBrMK7MJ","outputId":"c81e91f5-3a2e-43c9-a044-5c1e2f5940b0"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------+------------+-----------+-----+--------+----------+\n","|transaction_id|product_name|   category|price|quantity|sales_date|\n","+--------------+------------+-----------+-----+--------+----------+\n","|             1|       Apple|  Groceries|  0.5|      10|2023-09-01|\n","|             2|     T-shirt|   Clothing| 15.0|       2|2023-09-01|\n","|             3|    Notebook| Stationery|  2.0|       5|2023-09-02|\n","|             4|      Banana|  Groceries|  0.3|      12|2023-09-02|\n","|             5|      Laptop|Electronics|800.0|       1|2023-09-03|\n","|             6|       Pants|   Clothing| 25.0|       3|2023-09-03|\n","|             7|  Headphones|Electronics|100.0|       2|2023-09-04|\n","|             8|         Pen| Stationery|  1.0|      10|2023-09-04|\n","|             9|      Orange|  Groceries|  0.6|       8|2023-09-05|\n","|            10|    Sneakers|   Clothing| 50.0|       1|2023-09-05|\n","+--------------+------------+-----------+-----+--------+----------+\n","\n","root\n"," |-- transaction_id: integer (nullable = true)\n"," |-- product_name: string (nullable = true)\n"," |-- category: string (nullable = true)\n"," |-- price: double (nullable = true)\n"," |-- quantity: integer (nullable = true)\n"," |-- sales_date: string (nullable = true)\n","\n"]}],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType\n","from pyspark.sql.functions import col\n","\n","# Initialize Spark session\n","spark = SparkSession.builder.appName(\"Sales Data\").getOrCreate()\n","\n","# Define the data as a list of tuples\n","data = [\n","    (1, \"Apple\", \"Groceries\", 0.50, 10, \"2023-09-01\"),\n","    (2, \"T-shirt\", \"Clothing\", 15.00, 2, \"2023-09-01\"),\n","    (3, \"Notebook\", \"Stationery\", 2.00, 5, \"2023-09-02\"),\n","    (4, \"Banana\", \"Groceries\", 0.30, 12, \"2023-09-02\"),\n","    (5, \"Laptop\", \"Electronics\", 800.00, 1, \"2023-09-03\"),\n","    (6, \"Pants\", \"Clothing\", 25.00, 3, \"2023-09-03\"),\n","    (7, \"Headphones\", \"Electronics\", 100.00, 2, \"2023-09-04\"),\n","    (8, \"Pen\", \"Stationery\", 1.00, 10, \"2023-09-04\"),\n","    (9, \"Orange\", \"Groceries\", 0.60, 8, \"2023-09-05\"),\n","    (10, \"Sneakers\", \"Clothing\", 50.00, 1, \"2023-09-05\")\n","]\n","\n","# Define the schema for the DataFrame\n","schema = StructType([\n","    StructField(\"transaction_id\", IntegerType(), True),\n","    StructField(\"product_name\", StringType(), True),\n","    StructField(\"category\", StringType(), True),\n","    StructField(\"price\", DoubleType(), True),\n","    StructField(\"quantity\", IntegerType(), True),\n","    StructField(\"sales_date\", StringType(), True)\n","])\n","sales_df = spark.createDataFrame(data, schema)\n","sales_df.show()\n","sales_df.printSchema()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"MJ3BrKRcK9nF","outputId":"8dc00e8b-e2a4-4ffd-9e33-caa9acfc6cc9"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+------------------+\n","|   category|     total_revenue|\n","+-----------+------------------+\n","| Stationery|              20.0|\n","|  Groceries|13.399999999999999|\n","|Electronics|            1000.0|\n","|   Clothing|             155.0|\n","+-----------+------------------+\n","\n","+-----------+-------------+\n","|   category|total_revenue|\n","+-----------+-------------+\n","|Electronics|       1000.0|\n","|   Clothing|        155.0|\n","+-----------+-------------+\n","\n","+-----------+-------------+\n","|   category|max(quantity)|\n","+-----------+-------------+\n","| Stationery|           10|\n","|  Groceries|           12|\n","|Electronics|            2|\n","|   Clothing|            3|\n","+-----------+-------------+\n","\n","+------------+----------+\n","|product_name|sum(price)|\n","+------------+----------+\n","|     T-shirt|      15.0|\n","|      Banana|       0.3|\n","|      Laptop|     800.0|\n","|    Notebook|       2.0|\n","|       Apple|       0.5|\n","|    Sneakers|      50.0|\n","|      Orange|       0.6|\n","|         Pen|       1.0|\n","|       Pants|      25.0|\n","|  Headphones|     100.0|\n","+------------+----------+\n","\n","+-----------+------------------+\n","|   category|        avg(price)|\n","+-----------+------------------+\n","| Stationery|               1.5|\n","|  Groceries|0.4666666666666666|\n","|Electronics|             450.0|\n","|   Clothing|              30.0|\n","+-----------+------------------+\n","\n","+----------+-------------+\n","|sales_date|sum(quantity)|\n","+----------+-------------+\n","|2023-09-01|           12|\n","|2023-09-02|           17|\n","|2023-09-03|            4|\n","|2023-09-05|            9|\n","|2023-09-04|           12|\n","+----------+-------------+\n","\n","+-----------+----------+\n","|   category|min(price)|\n","+-----------+----------+\n","| Stationery|       1.0|\n","|  Groceries|       0.3|\n","|Electronics|     100.0|\n","|   Clothing|      15.0|\n","+-----------+----------+\n","\n","+----------+-----------+-------------+\n","|sales_date|   category|sum(quantity)|\n","+----------+-----------+-------------+\n","|2023-09-01|  Groceries|           10|\n","|2023-09-02|  Groceries|           12|\n","|2023-09-01|   Clothing|            2|\n","|2023-09-02| Stationery|            5|\n","|2023-09-03|Electronics|            1|\n","|2023-09-05|  Groceries|            8|\n","|2023-09-04| Stationery|           10|\n","|2023-09-04|Electronics|            2|\n","|2023-09-03|   Clothing|            3|\n","|2023-09-05|   Clothing|            1|\n","+----------+-----------+-------------+\n","\n","+--------------+------------+-----------+-----+--------+----------+----------------+\n","|transaction_id|product_name|   category|price|quantity|sales_date|discounted_price|\n","+--------------+------------+-----------+-----+--------+----------+----------------+\n","|             1|       Apple|  Groceries|  0.5|      10|2023-09-01|            0.45|\n","|             2|     T-shirt|   Clothing| 15.0|       2|2023-09-01|            13.5|\n","|             3|    Notebook| Stationery|  2.0|       5|2023-09-02|             1.8|\n","|             4|      Banana|  Groceries|  0.3|      12|2023-09-02|            0.27|\n","|             5|      Laptop|Electronics|800.0|       1|2023-09-03|           720.0|\n","|             6|       Pants|   Clothing| 25.0|       3|2023-09-03|            22.5|\n","|             7|  Headphones|Electronics|100.0|       2|2023-09-04|            90.0|\n","|             8|         Pen| Stationery|  1.0|      10|2023-09-04|             0.9|\n","|             9|      Orange|  Groceries|  0.6|       8|2023-09-05|            0.54|\n","|            10|    Sneakers|   Clothing| 50.0|       1|2023-09-05|            45.0|\n","+--------------+------------+-----------+-----+--------+----------+----------------+\n","\n"]}],"source":["\n","# 1. Calculate the Total Revenue per Category\n","total_revenue=sales_df.withColumn(\"rev\",col('price')*col(\"quantity\"))\n","total_revenue=total_revenue.groupBy(\"category\").agg(sum(\"rev\").alias(\"total_revenue\"))\n","total_revenue.show()\n","\n","# 2. Filter Transactions Where the Total Sales Amount is Greater Than $100\n","filter4=total_revenue.filter(total_revenue.total_revenue>100)\n","filter4.show()\n","\n","#3. Find the Most Sold Product\n","most_expensive_product=sales_df.groupBy(\"category\").max(\"quantity\")\n","most_expensive_product.show()\n","\n","#4. Top 3 grossers\n","top_three_grossers=sales_df.groupBy(\"product_name\").sum(\"price\")\n","top_three_grossers.show()\n","\n","#5. Calculate the Average Price per Product Category\n","avg_price=sales_df.groupBy(\"category\").avg(\"price\")\n","avg_price.show()\n","\n","#6. Calculate the Total Number of Items Sold per Day\n","total_items_sold=sales_df.groupBy(\"sales_date\").sum(\"quantity\")\n","total_items_sold.show()\n","\n","#7. Identify the Product with the Lowest Price in Each Category\n","lowest_price=sales_df.groupBy(\"category\").min(\"price\")\n","lowest_price.show()\n","\n","# 8. Calculate the Total Revenue for Each Product\n","total_revenue=sales_df.withColumn(\"rev\",col('price')*col(\"quantity\"))\n","total_revenue=total_revenue.groupBy(\"product_name\").agg(sum(\"rev\").alias(\"total_revenue\"))\n","\n","#9. Find the Total Sales per Day for Each Category\n","total_sales_per_day=sales_df.groupBy(\"sales_date\",\"category\").sum(\"quantity\")\n","total_sales_per_day.show()\n","\n","#10. Create a New Column for discounted price\n","discount_price=sales_df.withColumn(\"discounted_price\",col(\"price\")*0.9)\n","discount_price.show()"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPm4dxy2O8twyW+mJEs1VII"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}